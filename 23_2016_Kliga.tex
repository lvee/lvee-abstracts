\documentclass[10pt, a5paper]{article}
\input{preamble.tex}
\begin{document}
\title{Основные тенденции развития объектного хранилища данных Ceph}
\author{Александр Клыга, Minsk, Belarus}
\maketitle
\begin{abstract}
The new criteria of development of the distributed object data storage Ceph in addition to description of its basic components and their interaction with the Linux kernel will be presented. An interface between Ceph and ESOS in Data Storages System will be discussed as well.
\end{abstract}
Концепция объектного хранилища данных Ceph ~\cite{Kliga1} была разработана Сейджем Уэйлом в университете Калифорнии в Санта-Круз как часть его докторской диссертации в 2003 году. \linebreak Научно-исследовательский период разработки этой новой системы хранения данных и ее основных компонентов в рамках университета продолжился до конца 2006 когда, когда  под лицензией Lesser GNU Public License (LGPL) был открыт ее код, а к разработке подключились новые разработчики. Первый официальный релиз Ceph вышел в конце 2007 года, и до 2012 кода проект активно развивался и внедрялся на различных платформах, постепенно становясь ключевым решений для построения надежного хранилища данных.

Популярность проекту Ceph обеспечили несколько факторов. Первое, это открытость кода, что позволило быстро развивать проект привлекая в него новых разработчиков. Второе базовые принципы заложенные в Ceph позволяли создавать масштабируемое высоко отказоустойчивое  хранилище данных больших объемов (более подробно об этом в моем прошлом докладе «Обзор решений на рынке открытого ПО для создания СХД» ~\cite{Kliga2}). Третье были заключены  партнерские отношения с Canonical, RedHat и SUSE, что позволило поддерживать развитие проекта и его поддержку в дистрибутивах Linux. Четвертое тесное сотрудничество с сообществами облачных решений таких как OpenStack, CloudStack и OpenNebula позволило стать Ceph центральным хранилищем данных для них.

В 2012 году  Сейджем Уэйлом была основана компания  Inktank основной целью которой был выход на рынок программного обеспечения корпоративных клиентов для широкого внедрения решений на базе Ceph. Однако, несмотря  большую популяризацию  Ceph среди программных решений для объектных хранилищ данных, его внедрение в области крупных корпоративных пользователей был относительно невысок. Прежде всего это обуславливалось сильной конкуренцией со стороны ведущих поставщиков программных решений на базе открытого кода и слабостью собственной службы технической поддержки. В конечном итоге в конце апреля 2014 года компания Red Hat поглотила компанию Inktank, а развитие объектного хранилища данных Ceph пошло по новому пути ~\cite{Kliga3}. 
После того как компания Red Hat взяло под свой контроль все активы связанные с Ceph был разработан план развития системы, сформированы команды разработчиков и технической поддержки, а все наработки по Ceph включая  панель управления Ceph CALAMARI ~\cite{Kliga4} были переведены в open source, для того что бы ускорить процесс разработки.

План развития Ceph состоит из нескольких этапов по завершению которых должен быть представлен конечный продукт с длительным циклом поддержки. Основной тенденцией на первом этапе развития Ceph, как составляющей части продуктов компании RedHat, стала глубокое проработка структуры Ceph и ее компонентов и выработки обще концепции развития систем хранения данных компании и выпуск первого стабильного релиза.

В течении последующих двух лет  все составляющие компоненты Ceph были детально доработаны и переписаны, особое внимание было уделено интеграции с различными облачными платформами и кластерными системами хранения данных.  Итогом этой работы  стал выход нового полностью обновленного релиза Ceph V10.2.0 JEWEL представленного компанией RedHat 21-го апреля 2016 года ~\cite{Kliga5}, который был объявлен как стабильный и станет основой для  формирования новой ветки с длительным циклом поддержки (LTS).

Среди основных изменений в CephFS следует отметить:

\begin{itemize}
  \item в данном выпуске POSIX-совместимая CephFS объявлена стабильной, однако, некоторые функции отключены по умолчанию, в том числе снапшоты и конфигурация с несколькими активными серверами метаданных;
  \item утилита по восстановлению работы ФС после сбоя функционирует по полной.
  \item включена экспериментальная поддержка нескольких CephFS в пределах одного кластера.
\end{itemize}

В RBD (Ceph Block Device):

\begin{itemize}
  \item добавлена поддержка зеркалирования разделов (асинхронной репликации) с привлечением нескольких разных кластеров хранения;
  \item поддержка динамического управления включением или отключением таких возможностей как exclusive-lock, object-map, fast-diff, and journaling;
  \item добавлена возможность переименования снапшотов RBD;
  \item полностью переписан интерфейс командной строки, добавлена поддержка автодополнения ввода в bash.
\end{itemize}

В RADOS Gateway (RGW):

\begin{itemize}
  \item переписана и перепроектирована система межкластерного взаимодействия;
  \item добавлена экспериментальная поддержка доступа к данным через NFS;
  \item реализована поддержка протокола AWS4 и API OpenStack Keystone v3.
В RADOS (Reliable Autonomic Distributed Object Store):
  \item представлен новый более быстрый OSD-бэкенд BlueStore \linebreak (Object Storage Device);
  \item файловая система ext4 не рекомендована к использованию в качестве основной для Ceph OSD.
\end{itemize}

В настоящий момент было выпущено два релиза обновления Jewel, последний датируется 15 июнем 2016 года с номером  v10.2.2.
На втором этапе развития Ceph основной тенденцией станет выпуск и развитие новой платформы системы хранения данных на базе стабильного релиза (в настоящее время это v10.2.0 Jewel). Ее презентация состоялась относительно недавно 22 июля 2016 года, а новая платформа получила название Red Hat Ceph Storage 2 ~\cite{Kliga6}. Основным особенностями новой платформы являются:

\begin{itemize}
  \item работа с глобальными объектными кластерами, поддерживающими единое пространство имен и синхронизацию данных между кластерами, развернутыми в разных регионах;
  \item повышенная защищенность за счет интеграции с такими популярными системами аутентификации, как Active Directory, LDAP и OpenStack Identity (Keystone) v3;
  \item тесная совместимость со средами Amazon S3 и OpenStack \linebreak Object Storage (Swift), включая поддержку AWS v4 Client Signatures, версионность объектов и массовое их удаление.
\end{itemize}

Также в состав новой платформы хранения данных включена обновленная система мониторинга и управления хранилищем пришедшая на смену Ceph CALAMARI.

\begin{thebibliography}{99}
\bibitem{Kliga1} Ceph: the future of storage // ceph.com \url{http://ceph.com}}
\bibitem{Kliga2} А. Клыга. Обзор решений на рынке открытого ПО для создания СХД // Материалы конференции LVEE Winter 2016, Минск, 12-14 ферваля 2016 г. \url{https://lvee.org/ru/abstracts/181}
\bibitem{Kliga3} Red Hat Ceph Storage // Red Hat Ceph Storage \url{https://www.redhat.com/en/technologies/storage/ceph}
\bibitem{Kliga4} CEPH CALAMARI // Dashboard CEPH CALAMARI \url{http://ceph.com/community/ceph-calamari-goes-open-source/}
\bibitem{Kliga5} Ceph V10.2.0 JEWEL // Ceph V10.2.0 JEWEL \url{http://ceph.com/releases/v10-2-0-jewel-released/}
\bibitem{Kliga6} Red Hat Ceph Storage 2 // Red Hat Ceph Storage 2 \url{https://www.redhat.com/en/about/press-releases/red-hat-unveils-red-hat-ceph-storage-2-enhanced-}\linebreak\url{object-storage-capabilities-improved-ease-use}
\end{thebibliography}
\end{document}
