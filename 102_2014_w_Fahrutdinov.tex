\documentclass[10pt, a5paper]{article}
\input{preamble.tex}
\begin{document}
\title{Операционная система Linux как основа для построения высокопроизводительных систем хранения данных}
\author{Александр Фахрутдинов, Сызрань, РФ\footnote{\url{ar.fahrutdinov@gmail.com}, \url{http://lvee.org/en/abstracts/105}}}
\maketitle
\begin{abstract}
Report describes some Linux kernel subsystems which can be used to create a high-performance data storage. Well-known instruments of storage management (RAID and LVM) are explained as far as cutting-edge technologies for multi-level caching and data tiering. Also, kernel-mode virtual target device  for remote access to data storage from other hosts is reviewed.
\end{abstract}
Системы хранения данных (СХД) "--- это одна из основ современного мира компьютерных технологий. С возникновением облачных сред и повсеместном внедрением виртуализации возникла необходимость в сверхскоростных хранилищах большого объема и повышенной  отказоустойчивости. Кроме того, потребовались «умные» системы, которые хранят вместо несколько копий одних и тех же данных только одну, выделяют под данные именно столько реально имеющегося дискового пространства, сколько требуется, а не сколько запросит пользователь, умеют копировать массивы данных внутри устройства без отправки их на сервер и так далее.

Для организации эффективного хранения данных в Linux применяются виртуальные блочные устройства, которые представляют собой прослойку между собственно аппаратным хранилищем данных, например, жестким диском, и приложениями. Классический случай применения таких устройств программные дисковые массивы "--- RAID. За   обслуживание  RAID в Linux отвечает подсистема md (multiplie devices), которая позволяет создавать основные типы RAID "--- простое объединение дисков (JBOD), RAID уровней 0, 1 (зеркало), 4, 5, 6, а также "--- RAID 10.
.
RAID обеспечивает объединение дисков и отказоустойчивость в пределах одного сервера, однако он не решет проблему распределения пространства массива. Для управления дисковым пространством в Linux принято использовать менеджер виртуальных томов "--- LVM. Он позволяет гибко распределять место на дисках между приложениями, поддерживает создание, удаление и изменение размера тома «на лету», а также позволяет создавать собственные массивы типа JBOD,  RAID 0 и 1. Кроме того, LVM обеспечивает создание мгновенных снимков (snapshot) томов вне зависимости от того, поддерживает ли снимки файловая система тома, причем снимки доступны не только на чтение, но и на запись.  Однако платой за эту универсальность является низкое быстродействие снимков, что ограничивает область их применения.

В основе md и LVM лежит система проецирования устройств "--- device-mapper (dm). Она предоставляет единый механизм для создания на базе простых блочных устройств более сложных, наделенных дополнительными функциями. Возможности dm расширяются при помощи модулей, называемых «mapping targets». В стандартном ядре Linux, кроме md и LVM, присутствуют модули общего назначения для следующих целей:

\begin{itemize}
  \item диагностика и тестирование\begin{itemize}
  \item создание фиксированной задержки "--- dm-delay
  \item имитация сбоев "--- dm-flakey
  \item сбор статистики об обращениях к конкретным областям устройства "--- dm-statistics
  \item имитация пустого устройства "--- dm-zero
  \item проверка цифровой подписи тома "--- dm-verity
\end{itemize}


  \item построение RAID 0, 1 "--- dm-mod
  \item шифрование тома "--- dm-crypt
  \item доступ к подсистеме md для управления массивами через интерфейс device-mapper "--- dm-raid
\end{itemize}

Кроме того, в состав ядра входят модули, применяемые в высокопроизводительных СХД уровня предприятия

\begin{itemize}
  \item доступ к хранилищу через множество путей\begin{itemize}
  \item с простым резервированием "--- dm-multipath
  \item с учетом длины очереди "--- dm-queue-length
  \item с учетом времени доступа "--- dm-service-time
  \item доступ к разным регионам хранилища через разные пути "--- dm-switch
\end{itemize}


  \item кэширование данных на твердотельных накопителях\begin{itemize}
  \item dm-cache (с версии ядра 3.9,  релиз 28 апреля 2013)
  \item bcache (с версии 3.10, релиз 30 июня 2013)
  \item flashcache (разработка Facebook, не включен в ядро)
\end{itemize}


  \item не включены в ядро, но могут быть полезны:\begin{itemize}
  \item «многослойное» хранилище из разных типов дисков "--- btier
  \item RAM-диск с периодическим сохранением информации "--- eprd
Как известно, в Linux кэшируется только доступ к файловой системе, доступ же напрямую к блочным устройствам не кэшируется, поэтому одним из способов повысить быстродействие, особенно в может быть применение указанных выше модулей. В особенности это справедливо для систем виртуализации.
\end{itemize}


\end{itemize}

В системах  виртуализации принято выделять дисковое пространство не на этапе создания виртуальной машины, а по мере необходимости (thin provisioning). Device mapper имеет подобную функциональность начиная с ядра 3.2 (релиз 4 января 2012). Модуль dm-thin-pool позволяет создавать виртуальные устройства, которые не резервируют весь предназначенный им объем в момент создания, а расходуют выделенное физическое пространство по мере заполнения самого устройства данными. Также dm-thin-pool поддерживает возврат более не используемых блоков в общий пул, если вышележащая файловая система уведомит его об этом. Кроме того, модуль dm-thin-pool  позволяет создавать виртуальное устройство на базе шаблона, в роли которого выступает другое устройство, доступное только для чтения.

Несмотря на то, что device-mapper обладает широкой функциональностью, нельзя не заметить, что его назначение "--- создание виртуальных устройств «высокого уровня», которые служат для управления уже имеющимся дисковым пространством и должны опираться на программный или аппаратный RAID. В то же время  device-mapper не обеспечивает доступ к созданному устройству за пределами хоста, например, по протоколам iSCSI и FC. Для этих целей в ядро Linux не так давно была включена инфраструктура LIO-target.

LIO-target "--- это разработка компании Rising Tide Systems, которая была лицензирована под  GPL и включена в ядро Linux, начиная с версии 2.6.38 (релиз 15 января 2011 г.). В 2013 году разработчики LIO-target покинули  Rising Tide Systems и основали компанию Datera, целью которой является разработка программного обеспечения для СХД.

LIO-target состоит из высокоскоростного виртуального блочного устройства с поддержкой расширенного набора команд SCSI, драйверов нижнего уровня (бэкэндов), при помощи которых виртуальное устройство отображается на реальное и драйверов верхнего уровня (фронтэндов), при помощи которых можно получить доступ к устройству из-за пределов хоста.
Поддерживаются следующие бэкэнды:

\begin{itemize}
  \item FILEIO "--- обращение к нижележащему блочному устройству как к файлу через слой виртуальной ФС.
  \item BLOCKIO "--- обращение к нижележащему блочному устройству при помощи команд SCSI.
  \item PSCSI "--- пересылка команд  SCSI физическому   устройству, например, RAID-контроллеру, без обработки.
  \item Memory Copy RAMDISK "--- блочное устройство в оперативной памяти.
Фронтэнды обеспечивают подключение к LIO-target других хостов. В настоящий момент поддерживаются интерфейсы iSCSI, FCoE, Fibre Channel, InfiniBand, IBM vSCSI, FireWare и USB. Кроме того, возможна эмуляция блочного устройства на локальной машине, а также передача такого устройства внутрь виртуальной машины под управлением \linebreak KVM (vHost).
\end{itemize}

Особенностью  LIO-target является поддержка SCSI-команд аппаратного ускорения для систем хранения данных (VAAI). Эти команды используются, в первую очередь, системами виртуализации и призваны разгрузить гипервизор при таких ресурсоемких операциях, как клонирование виртуальной машины. До недавнего времени этот набор команд был реализован только в коммерческих СХД, теперь же он доступен всем пользователям ОС Linux.

Таким образом, ОС Linux предоставляет функциональность, достаточную для построения на базе конкретной аппаратной платформы надежного и высокопроизводительного хранилища, которое может быть использовано как само по себе, так и в качестве узла в распределенной системе хранения данных.

\end{document}
